{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important portion of the effort behind building a new transformer model is creating the new model tokenizer. The tokenizer is our translator from human-readable text, to transformer readable tokens. In this article, we will learn exactly how to build our own transformer tokenizer.\n",
    "\n",
    "In this tutorial we will use the multi-lingual OSCAR dataset from huggingface to train a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds = datasets.list_datasets()\n",
    "print(\"total number of datasets\", len(all_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the OSCAR dataset\n",
    "dataset = datasets.load_dataset('oscar', 'unshuffled_deduplicated_la')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First record\n",
    "# From here we can see that the Latin subset contains 18.8K samples, where each sample is a dictionary containing an id and text.\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store all of our samples in plain text files, separating each sample by a newline character.\n",
    "We will split each text file into chunks of 5K samples each (although not necessary with a dataset of this size — it’s required for large datasets) and save them into a new oscar_la directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(dataset['train']):\n",
    "    # remove newline characters from each sample as we need to use exclusively as seperators\n",
    "    sample = sample['text'].replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    if len(text_data) == 5_000:\n",
    "        # once we hit the 5K mark, save to file\n",
    "        with open(f'data/oscar_la/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "\n",
    "# after saving in 5K chunks, we will have ~3808 leftover samples, we save those now too\n",
    "with open(f'data/oscar_la/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Latin roBERTa Tokenizer\n",
    "\n",
    "To train the tokenizer we make use of the Byte pair encoding (not specific to NLP - its a compression algorithm)\n",
    "\n",
    "Byte-level encoding means we will be building our tokenizer vocabulary from an alphabet of bytes. Thanks to this, all words will be decomposable into tokens — even new words — and so we will not need special unknown tokens.\n",
    "\n",
    "We need a list of files to feed into our tokenizer’s training process, we will list all .txt files from our oscar_la directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files\n",
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path('data/oscar_la').glob('**/*.txt')]\n",
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use roBERTa special tokens, a vocabulary size of 30522 tokens, and a minimum frequency (number of times a token appears in the data for us to take notice) of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "\n",
    "# initialize\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# and train\n",
    "tokenizer.train(files=paths, \n",
    "                vocab_size=30_522, \n",
    "                min_frequency=2, # number of times a token appears in the data\n",
    "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenizer\n",
    "tk_path = 'models/bertius'\n",
    "os.mkdir(tk_path)\n",
    "\n",
    "tokenizer.save_model(tk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tk_path = 'models/bertius'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "lorem_ipsum = (\n",
    "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor \"\n",
    "    \"incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud \"\n",
    "    \"exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute \"\n",
    "    \"irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla \"\n",
    "    \"pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia \"\n",
    "    \"deserunt mollit anim id est laborum.\"\n",
    ")\n",
    "\n",
    "lorem_ipsum = (\n",
    "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor \"\n",
    "    \"incididunt ut labore et dolore magna aliqua.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll include the typical padding/truncation\n",
    "lorem_tk = tokenizer(lorem_ipsum, \n",
    "                     max_length=512, \n",
    "                     padding='max_length', \n",
    "                     truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem_tk.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see our two tensors, input_ids and attention_mask. In input_ids we can see our start of sequence token __<*s*>__ represented by 0, the end of sequence token <s\\\\> represented by 2, and padding tokens <*pad*> represented by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem_tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a transformer from scratch\n",
    "1. [Implementing the Transformer Encoder from Scratch in TensorFlow and Keras](https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking)\n",
    "2. [Implementing the Transformer Decoder from Scratch in TensorFlow and Keras](https://machinelearningmastery.com/implementing-the-transformer-decoder-from-scratch-in-tensorflow-and-keras/)\n",
    "3. [Training the Transformer Model](https://machinelearningmastery.com/training-the-transformer-model/)\n",
    "4. [Building Transformer Models with Attention](https://machinelearningmastery.com/building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator-in-12-days/)\n",
    "5. [Training Compact Transformers from Scratch in 30 Minutes with PyTorch](https://medium.com/pytorch/training-compact-transformers-from-scratch-in-30-minutes-with-pytorch-ff5c21668ed5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 2 \n",
    "### Due: August 27, 2023\n",
    "In this assignment, you will construct a language models for any of the African language from [CC-100](https://data.statmt.org/cc-100/) using the huggginface platform. Provide the following information\n",
    "1. Ensure you follow the appropriate machine learning paradigm to train your model\n",
    "2. define/us an appropriate metric to evaluate the performance of the language model\n",
    "3. Perform error analysis and give reasons for the limitations of your model if any.\n",
    "\n",
    "Data source: [CC-100: Monolingual Datasets from Web Crawl Data](https://data.statmt.org/cc-100/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
